# -*- coding: utf-8 -*-
"""Resume_screening_app NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iv4KLhImbaixrw4bmuzULHs8fkH77g3p
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/UpdatedResumeDataSet.csv')

df.head()

df['Category'].value_counts()

# Plotting graph for the above data
sns.countplot(df['Category'])

# Exploring Resume

df['Category'][0]

df['Resume'][0]

"""# **Text Cleaning**
*  URL's
*  hastags
*  Mentions
*  special characters
*  punctuations
"""

import re

def cleanResume(txt):
    cleanText = re.sub(r'http\S+\s', ' ', txt) # remove URL
    cleanText = re.sub(r'RT|cc', ' ', cleanText) # remove RT and cc
    cleanText = re.sub(r'#\S+\s', ' ', cleanText) # Remove hashtags
    cleanText = re.sub(r'@\S+', ' ', cleanText) # Remove mentions (@usernames or emails)
    cleanText = re.sub(r'[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"""), ' ', cleanText) # remove punctuation and special characters
    cleanText = re.sub(r'[^\x00-\x7f]', ' ', cleanText) # remove no ASCII characters
    cleanText = re.sub(r'\s+', ' ', cleanText) # Remove space
    return cleanText.strip() # Remove or Trim Spaces

cleanResume("Check ### ## this  link https://example.com and also http://test.org for more info.and at @gmail.com")

df['Resume'] = df['Resume'].apply(lambda x : cleanResume(x))

df['Resume'][0]

"""# **Word in to Categorical Values**"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

le.fit(df['Category'])
df['Category'] = le.transform(df['Category'])

print(df['Category'])

"""**Vectorization**"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english')

tfidf.fit(df['Resume'])
req_txt = tfidf.transform(df['Resume'])

"""**Splitting**"""

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(req_txt,df['Category'],test_size=0.2,random_state=42)

x_train.shape

df['Category'].unique()

"""# **Now let's train the model and print the classification report**"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

# Ensure that x_train and x_test are dense if they are sparse

x_train = x_train.toarray() if hasattr(x_train,'toarray') else x_train
x_test = x_test.toarray() if hasattr(x_test,'toarray') else x_test

# Train Kneighbors classifier

clf = OneVsRestClassifier(KNeighborsClassifier())
clf.fit(x_train,y_train)
y_pred = clf.predict(x_test)

print("\n KneighborsClassifiers Results:")
print(f"Accuracy:{accuracy_score(y_test,y_pred):.4f}")
print(f"Confusionmatrix:\n{confusion_matrix(y_test,y_pred)}")
print(f"Classification Report:\n{classification_report(y_test, y_pred)}") # Faster training and slower prediction

# Train SVC
svc_model = OneVsRestClassifier(SVC())
svc_model.fit(x_train,y_train)
y_pred_svc = svc_model.predict(x_test)
print("SVC result:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svc):.4f}")
print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred_svc)}")
print(f"Classification Report:\n{classification_report(y_test, y_pred_svc)}")  # slow training and faster prediction

# 3. Train RandomForestClassifier

rf_model = OneVsRestClassifier(RandomForestClassifier())
rf_model.fit(x_train,y_train)
y_pred_rf = rf_model.predict(x_test)

print("\nRandomForestClassifier Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred_rf)}")
print(f"Classification Report:\n{classification_report(y_test, y_pred_rf)}")

"""# **Prediction System**"""

import pickle
pickle.dump(tfidf,open('tfidf.pkl','wb'))
pickle.dump(clf,open('clf.pkl','wb'))

myresume = """I am a data scientist specializing in machine
learning, deep learning, and computer vision. With
a strong background in mathematics, statistics,
and programming, I am passionate about
uncovering hidden patterns and insights in data.
I have extensive experience in developing
predictive models, implementing deep learning
algorithms, and designing computer vision
systems. My technical skills include proficiency in
Python, Sklearn, TensorFlow, and PyTorch.
What sets me apart is my ability to effectively
communicate complex concepts to diverse
audiences. I excel in translating technical insights
into actionable recommendations that drive
informed decision-making.
If you're looking for a dedicated and versatile data
scientist to collaborate on impactful projects, I am
eager to contribute my expertise. Let's harness the
power of data together to unlock new possibilities
and shape a better future.
Contact & Sources
Email: 611noorsaeed@gmail.com
Phone: 03442826192
Github: https://github.com/611noorsaeed
Linkdin: https://www.linkedin.com/in/noor-saeed654a23263/
Blogs: https://medium.com/@611noorsaeed
Youtube: Artificial Intelligence
ABOUT ME
WORK EXPERIENCE
SKILLES
NOOR SAEED
LANGUAGES
English
Urdu
Hindi
I am a versatile data scientist with expertise in a wide
range of projects, including machine learning,
recommendation systems, deep learning, and computer
vision. Throughout my career, I have successfully
developed and deployed various machine learning models
to solve complex problems and drive data-driven
decision-making
Machine Learnine
Deep Learning
Computer Vision
Recommendation Systems
Data Visualization
Programming Languages (Python, SQL)
Data Preprocessing and Feature Engineering
Model Evaluation and Deployment
Statistical Analysis
Communication and Collaboration
"""

import pickle

# Load the trained classifier
clf = pickle.load(open('clf.pkl','rb'))

# clean the input  resume
cleaned_resume = cleanResume(myresume)

# Transform the cleaned resume usign the trained TFIDF vectorizer
input_features = tfidf.transform([cleaned_resume])

# Make the prediction using the loaded classifier
prediction_id = clf.predict(input_features)[0]

# MAP categoryID to category name
category_mapping = {
    15 : "Java Developer",
    23 : "Testing",
    8 : "DevOps Engineer",
    20: "python Developer",
    24 : "Web Designing",
    12 :  "HR",
    13 :  "Hadoop",
    3 : "Blockchain",
    10 : "ETL Developer",
    18 : "Operations Manager",
    6 : "Data Science",
    22  : "Sales",
    16 : "Mechanical Engineer",
    1 : "Arts",
    7 : "Database",
    11 : "Electrical Engineering",
    14 : "Health and fitness",
    19 : "PMO",
    4: "Business Analyst",
    9 : "DotNet developer",
    2 : "Automation Testing",
    17 : "Network Security Engineer",
    21 : "SAP Developer",
    5 : "Civil Engineer",
    0 : "Advocate",


}
category_name = category_mapping.get(prediction_id,"unknown")
print("predicted_Category:",category_name)
print(prediction_id)

print(prediction_id)

"""# **App**"""

!pip install streamlit
!pip install scikit-learn
!pip install python-docx
!pip install PyPDF2

import streamlit as st
import pickle
import nltk


nltk.download('punkt')
nltk.download('stopwords')

# Loading Models

clf = pickle.load(open('clf.pkl','rb'))
tfidf = pickle.load(open('tfidf.pkl','rb'))

"""# **Web App**"""

st.title('Resume Screening')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pickle
# import re
# 
# # --- 1. Use the EXACT same cleaning function from your training notebook ---
# def cleanResume(txt):
#     cleanText = re.sub(r'http\\S+\\s', ' ', txt)
#     cleanText = re.sub(r'RT|cc', ' ', cleanText)
#     cleanText = re.sub(r'#\\S+\\s', ' ', cleanText)
#     cleanText = re.sub(r'@\\S+', ' ', cleanText)
#     cleanText = re.sub(r'[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"""), ' ', cleanText)
#     cleanText = re.sub(r'[^\x00-\x7f]', ' ', cleanText)
#     cleanText = re.sub(r'\\s+', ' ', cleanText)
#     return cleanText.strip()
# 
# # Loading your saved models
# try:
#     tfidf = pickle.load(open('tfidf.pkl','rb'))
#     clf = pickle.load(open('clf.pkl','rb'))
# except FileNotFoundError:
#     st.error("Model files not found! Please make sure 'tfidf.pkl' and 'clf.pkl' are uploaded.")
#     st.stop()
# 
# # --- 2. Create the dictionary to map prediction IDs to category names ---
# category_mapping = {
#     15: "Java Developer", 23: "Testing", 8: "DevOps Engineer",
#     20: "Python Developer", 24: "Web Designing", 12: "HR",
#     13: "Hadoop", 3: "Blockchain", 10: "ETL Developer",
#     18: "Operations Manager", 6: "Data Science", 22: "Sales",
#     16: "Mechanical Engineer", 1: "Arts", 7: "Database",
#     11: "Electrical Engineering", 14: "Health and fitness",
#     19: "PMO", 4: "Business Analyst", 9: "DotNet Developer",
#     2: "Automation Testing", 17: "Network Security Engineer",
#     21: "SAP Developer", 5: "Civil Engineer", 0: "Advocate"
# }
# 
# # --- Streamlit App ---
# st.title('ðŸ“„ Resume Category Prediction')
# st.markdown("Enter the text from a resume to predict its category.")
# 
# input_resume = st.text_area('Paste Resume Text Here', height=300)
# 
# if st.button('Predict Category'):
#     if input_resume:
#         # 1. Clean the input resume
#         cleaned_resume = cleanResume(input_resume)
# 
#         # 2. Vectorize the cleaned resume
#         vector_input = tfidf.transform([cleaned_resume])
# 
#         # 3. Predict the category ID
#         prediction_id = clf.predict(vector_input)[0]
# 
#         # 4. Map the ID to the category name
#         category_name = category_mapping.get(prediction_id, "Unknown Category")
# 
#         # 5. Display the result
#         st.success(f'This resume is predicted to be in the **{category_name}** category.')
#     else:
#         st.error("Please paste some resume text before predicting.")

from pyngrok import ngrok

# Terminate any existing ngrok tunnels
ngrok.kill()

# Set your ngrok authtoken
NGROK_AUTH_TOKEN = "34BooR5jvbwMheygZJKSE8FqZ58_6PpDgxoiPB7xgbbYAgJv6" # <--- PASTE YOUR TOKEN HERE
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Open a tunnel to the streamlit port (8501)
public_url = ngrok.connect(8501)
print(f"Click this link to view your app: {public_url}")

# Run the streamlit app
!streamlit run app.py



